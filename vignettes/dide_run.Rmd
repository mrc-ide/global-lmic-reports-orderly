---
title: "Running Fits on DIDE Cluster"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Running Fits on DIDE Cluster}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 8, collapse = TRUE)
knitr::opts_knit$set(root.dir = here::here())
library(tidyverse)
#source all file in R directory
walk(list.files(here::here("R"), full.names = TRUE), source)
```

# Overview

This Rmd document gives an overview of the steps to be taken to run model fits
on the DIDE HPC cluster, check results before collating and pushing to github. 

As a background, the fits used to be run through a series of tasks in specific
docker containers. Since, then we have switched from our own cluster to the 
departmental cluster.

All code chunks are set to not eval so the knitted html is just for display purposes. 
I would recommend stepping through this Rmd each time to rerun the fits, updating
parameters etc as and where needed. Sections with bash code are commented out and can
be run from within the Rmd script if bash is correctly set up, and if not then can be
run from the RStudio Terminal if set up to use Git Bash as the default for the
terminal (see Tools > Global Options > Terminal > New Terminals Open With)  

\

# Set up

All tasks for running the model fits are conducted using orderly. This includes:

1. *parameters_vaccines*

This task runs the fitting process to calculate booster model vaccine efficacies.
This task only needs to be run once or any time the task it update (i.e. due to
new variants or changes to the model). The fitting process is unreliable so any
new runs should be checked via the calibration.pdf file.

2. *input_jhu*

Gathers data on reported COVID cases and deaths from [John Hopkins Coronavirus Resource Centre](https://coronavirus.jhu.edu/map.html) and [Worldometer](https://www.worldometers.info/coronavirus/).

3. *input_excess_mortality*

This task gathers the excess mortality data and estimates from The Economist's
[excess mortality model](https://github.com/TheEconomist/covid-19-the-economist-global-excess-deaths-model).

4. *input_vaccinations*

This task gathers and formats country specific vaccination data, getting data
from [OWID](https://ourworldindata.org/covid-vaccinations) and the [WHO](https://covid19.who.int/).

5. *input_sequencing*

This estimates the timings of the variants in each country using open source 
sequencing data from [NextStrain](https://nextstrain.org/). Can use data from 
[GISAID](https://www.gisaid.org/) provided that valid credentials are placed in
the `src/inputs_sequencing/gisaid_credentials.secret`. This file should be ignored
by *git* though take care not to send the file to anybody.


These tasks 2-5 form the initial tasks that ensure all the correct data is ready
before running the actual model fits. To run these tasks, we simply need to provide
what the `date` is that we are conducting model fits for, i.e. up to what date are
we running.

```{r, eval = FALSE}
date <- "2022-09-01"

glodide_loc <- "N:/lmic_fitting/glodide" ###Update if on a different drive!
```

These arguments are then passed to our tasks:

```{r eval=FALSE, message=FALSE, warning=FALSE}
# message("*** VE Parameters (Please check calibration.pdf)!")
# parameters_vaccines_id <- orderly::orderly_run("parameters_vaccines", echo = FALSE)
# orderly::orderly_commit(parameters_vaccines_id)

message("*** Reported COVID Deaths and Cases")
input_jhu_id <- orderly::orderly_run("input_jhu", parameters = list(date=date), echo = FALSE)
orderly::orderly_commit(input_jhu_id)

message("*** Excess Mortality")
input_excess_mortality_id <- orderly::orderly_run("input_excess_mortality", parameters = list(date=date), echo = FALSE)
orderly::orderly_commit(input_excess_mortality_id)

message("*** Vaccination Data")
input_vaccinations_id <- orderly::orderly_run("input_vaccinations", parameters = list(date=date), echo = FALSE)
orderly::orderly_commit(input_vaccinations_id)

message("*** Variant Timings")
input_sequencing_id <- orderly::orderly_run("input_sequencing", parameters = list(date=date, gisaid=TRUE), echo = FALSE)
orderly::orderly_commit(input_sequencing_id)
```

With these tasks run, we now begin by creating our model fit tasks. Throughout
the pandemic, we have updated the model fit tasks to include new data, new models
etc. Each of the various model fit tasks starts `lmic_reports_`, but the current 
model task is `lmic_reports_rt_optimise`. 

We run the the model fits on the DIDE HPC cluster. To do this we first bundle the 
orderly reports to be run using `orderly::orderly_bundle_pack`. The zip files
generated from this are then copied to the `glodide` repository, which is used to 
submit these tasks. 

```{r eval=FALSE, message=FALSE, warning = FALSE}

message("*** Creating country task bundles")

# main input parameters

task <- "lmic_reports_rt_optimise"
parameters <- list(
  date = date,
  samples = 32*3, #how many random samples to generate and fit
  seed = FALSE, #Set a seed, useful for debugging
  parallel = TRUE,
  #Should we build the required documentation+data for the fit
  document = TRUE
  # #Fitting parameters (leave as blank to use defaults in fitting_params.Rds)
  # initial_infections_interval = c(5, 500),
  # n_particles = 10,
  # k = 14,
  # rt_interval = c(0.5, 10)
)

# get the isos
iso3cs <- grep('^[A-Z]{3}\\s*', readLines(file.path(here::here(), "countries")), value = TRUE)

# make the orderly bundles to be run on the cluster
path_bundles <- file.path(
  glodide_loc, "analysis", "data", "raw", date
)
dir.create(path_bundles, showWarnings = FALSE, recursive = TRUE)

# bundle these up - this will take like 10 mins to create all the zips. 
bundles <- lapply(
  iso3cs, function(x) {
    #set country in parameters
    parameters$iso3c = x
    #pack bundle
    orderly::orderly_bundle_pack(
      path = path_bundles,
      name = task,
      parameters = parameters
    )
  }
 )

# now label these with the iso3cs and save the file paths
names(bundles) <- iso3cs
saveRDS(bundles, file.path(path_bundles, "bundles.rds"))
```

This will create an orderly task for each country that is then saved in the 
location on the server where they are to be run. At this point, open the 
`glodide` repository on the server and run through the submission script to submit
these to the cluster:

```{r eval=FALSE}
system(paste0("open ", "\"", glodide_loc, "/glodide.Rproj", "\""))
```

----

After the tasks have all finished, we can check to see the fits to see if they are good:  

\

# Check the fits

To do this we take the path of the finished tasks from our bundles

```{r eval=FALSE, message=FALSE, warning = FALSE}
# use the bundles paths to work out the path to the runs in derived
paths <- gsub("raw", "derived", vapply(bundles, FUN = "[[", FUN.VALUE = character(1), "path"))

# now extract the fitting.pdf files
td <- tempdir(check = TRUE)
fits <- lapply(paths, function(x) {
  if(file.exists(x)){
    zip::unzip(
      zipfile = x, 
      files = file.path(gsub("\\.zip", "", basename(x)), c("pack/excess_fitting.pdf", "pack/reported_fitting.pdf")), 
      exdir = td
    )
  }
})

# get the filepaths for these 
pdfs <- grep("fitting", list.files(td, full.names = TRUE, recursive = TRUE), value = TRUE)

# combine the files that are larger than 0b. Ob files are for countries that have
# no COVID-19 deaths to date and as such don't have a fitting.pdf but this file is
# created because it needs to be for orderly to finish the task
qpdf::pdf_combine(
  input = pdfs[file.size(pdfs) > 0], 
  output = here::here("fits", paste0("lmic_reports_rt_optimise_", date, ".pdf"))
)
```

Now we can view these to work out if they look good. See the troubleshoot for more
info on what to look out for etc. If some countries need to be rerun, then work out
which countries require rerunning and re-bundle those to be run again:

```{r eval=FALSE, message=FALSE, warning = FALSE}
# bundle the countries we need to rerun
iso3cs_to_rerun <- c()
#change start date
bundles_to_rerun <- lapply(
  iso3cs_to_rerun,
  function(x) {
    #set country in parameters
    parameters$iso3c = x
    orderly::orderly_bundle_pack(
      path = path_bundles,
      name = task,
      parameters = parameters
    )
  }
)

names(bundles_to_rerun) <- iso3cs_to_rerun
saveRDS(bundles_to_rerun, file.path(path_bundles, "bundles_to_rerun.rds"))
```

Then go to `glodide` and resubmit these and repeat above to see the fits. If they
are better then replace the bundle paths in `bundles` object so we know which 
are the correct tasks to then pull back down.  

\

# Pull fits back into orderly

Once we are happy with a set of fits for each country, we need to pull them back
down off the server to local.

First read in the paths of the original bundles

```{r eval=FALSE, message=FALSE, warning = FALSE}
# get the original bundles
bundles <- readRDS(file.path(path_bundles, "bundles.rds"))
```

Depending on whether any needed to be rerun, the below will be needed to 
replace paths in bundles, with the file paths of any new runs:

```{r eval=FALSE, message=FALSE, warning = FALSE}
# any that we had to rerun
rerun <-  readRDS(file.path(path_bundles, "bundles_to_rerun.rds"))

# replace the ones we rerun within bundles
nms_ch <- c(names(rerun))
bundles[nms_ch] <- c(rerun)

# now get the filepaths of the tasks to import back
tasks <- gsub("raw", "derived", as.character(vapply(bundles, "[[", character(1), "path")))
```

Now that we have updated with the rerun bundles, we know import these back into 
our local `orderly` archive:

```{r eval=FALSE, message=FALSE, warning = FALSE}
import <- map(tasks, ~tryCatch(orderly::orderly_bundle_import(.x), error = function(e){NULL}))
# just double check they all imported
all(unlist(import))
```

# Create the changes to the github pages

## Get gh-pages, gh-fits, gh-esft

The Github pages repository is at [www.github.com:mrc-ide/global-lmic-reports](www.github.com:mrc-ide/global-lmic-reports) and
contains all the outputs of model fits conducted to date and the web pages to describe the model fits for each country. It is
cloned into `gh-pages`, which is where we collate the outputs of the model fits into.

We also have a similar repo to collect the fitted model objects for other modelling work at, [https://github.com/mrc-ide/nimue_global_fits](https://github.com/mrc-ide/nimue_global_fits), which is stored in subdirectory `gh-fits`.

If any of these are not in the repo, i.e. you have just clones `global-lmic-reports-orderly` down for example, then you need to run following in terminal (or from within this repo if the bash chunk tag is working correctly).

```{bash}
mkdir gh-pages
git clone git@github.com:mrc-ide/global-lmic-reports.git gh-pages
mkdir gh-fits
git clone git@github.com:mrc-ide/nimue_global_fits.git gh-fits
#mkdir gh-esft
#git clone git@github.com:mrc-ide/global_lmic_projections_esft.git gh-esft
```

## Update the github fits

These scripts update the fits in `gh-fits` and pushes the `gh-fits` changes to GitHub by calling a `.sh` file.
Potentially worth running the `.sh` script from the terminal or git bash to view the log.

```{r eval=FALSE, message=FALSE, warning = FALSE}
#just takes the latest fit for each country, will take some time to keep RAM usage low
report_ids <- gather_report_ids(task)
update_gh_fits(report_ids, "grid_out.Rds")

#requires git bash be on the path and that the working directory is set correctly
#shell(paste0(here::here("scripts/update_github_fits.sh"), " ", date), intern = TRUE, wait = TRUE, shell = "bash")
#alternatively just copy this code into git bash
message("Paste me into git bash!")
dput(paste0("scripts/update_github_fits.sh", " ", date))
```

## The collation step for run_collate_vaccine

Next we run the collation step, where the necessary outputs from the model fits are copied to gh-pages. It conducts the following steps:

1. Fetches the latest run model fit for each country for the date argument provided. 
2. Copies the pdfs/html files over to `gh-pages`
3. Copies the projections from each model fit over and combines these into a combined data zip
4. Copies model projections to the `index_page` and `regional_page` tasks
5. Runs the remaining tasks needed to create the website (index, FAQ, regional pages)
6. Copies the output from these remaining tasks to gh-pages. 

Either execute this though the R code below, and push to the GitHub repo. We
do this in two steps as the git pack size if all done at once is too big.

```{r eval=FALSE, message=FALSE, warning = FALSE}
update_gh_pages(report_ids, date)

#requires git bash be on the path and that the working directory is set correctly
message("Paste me into git bash!")
dput("scripts/update_github_pages.sh")
```
